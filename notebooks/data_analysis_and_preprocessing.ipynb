{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142d637e",
   "metadata": {},
   "source": [
    "# Task 1: Data Analysis and Preprocessing\n",
    "\n",
    "This notebook covers:\n",
    "- Handling missing values\n",
    "- Data cleaning\n",
    "- Exploratory Data Analysis (EDA)\n",
    "- Geolocation merging\n",
    "- Feature engineering\n",
    "- Data transformation (class imbalance, scaling, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4fec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import ipaddress\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073f88b0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941a5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_df = pd.read_csv('../data/Fraud_Data.csv')\n",
    "ip_df = pd.read_csv('../data/IpAddress_to_Country.csv')\n",
    "cc_df = pd.read_csv('../data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20cc152",
   "metadata": {},
   "source": [
    "## Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5652f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "print('Fraud_Data missing values:')\n",
    "print(fraud_df.isnull().sum())\n",
    "print('\\nIpAddress_to_Country missing values:')\n",
    "print(ip_df.isnull().sum())\n",
    "print('\\nCreditcard missing values:')\n",
    "print(cc_df.isnull().sum())\n",
    "\n",
    "# Drop rows with missing values for simplicity\n",
    "fraud_df = fraud_df.dropna()\n",
    "ip_df = ip_df.dropna()\n",
    "cc_df = cc_df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76af266",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46d5daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "fraud_df = fraud_df.drop_duplicates()\n",
    "ip_df = ip_df.drop_duplicates()\n",
    "cc_df = cc_df.drop_duplicates()\n",
    "\n",
    "# Correct data types\n",
    "fraud_df['signup_time'] = pd.to_datetime(fraud_df['signup_time'])\n",
    "fraud_df['purchase_time'] = pd.to_datetime(fraud_df['purchase_time'])\n",
    "fraud_df['age'] = fraud_df['age'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e4c5f9",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b177a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate analysis\n",
    "plt.figure(figsize=(6,4))\n",
    "fraud_df['purchase_value'].hist(bins=50)\n",
    "plt.title('Purchase Value Distribution')\n",
    "plt.xlabel('Purchase Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Bivariate analysis\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.boxplot(x='class', y='purchase_value', data=fraud_df)\n",
    "plt.title('Purchase Value by Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0988dd1f",
   "metadata": {},
   "source": [
    "## Merge Datasets for Geolocation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d5da2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert IP addresses to integer\n",
    "def ip_to_int(ip):\n",
    "    try:\n",
    "        return int(ipaddress.IPv4Address(ip))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "fraud_df['ip_int'] = fraud_df['ip_address'].apply(ip_to_int)\n",
    "ip_df['lower_bound_ip_address'] = ip_df['lower_bound_ip_address'].apply(ip_to_int)\n",
    "ip_df['upper_bound_ip_address'] = ip_df['upper_bound_ip_address'].apply(ip_to_int)\n",
    "\n",
    "# Merge: assign country to each transaction\n",
    "def find_country(ip):\n",
    "    row = ip_df[(ip_df['lower_bound_ip_address'] <= ip) & (ip_df['upper_bound_ip_address'] >= ip)]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['country']\n",
    "    return 'Unknown'\n",
    "\n",
    "fraud_df['country'] = fraud_df['ip_int'].apply(find_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785df655",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ddeab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction frequency per user\n",
    "user_freq = fraud_df.groupby('user_id').size().rename('transaction_count')\n",
    "fraud_df = fraud_df.merge(user_freq, on='user_id')\n",
    "\n",
    "# Time-based features\n",
    "fraud_df['hour_of_day'] = fraud_df['purchase_time'].dt.hour\n",
    "fraud_df['day_of_week'] = fraud_df['purchase_time'].dt.dayofweek\n",
    "fraud_df['time_since_signup'] = (fraud_df['purchase_time'] - fraud_df['signup_time']).dt.total_seconds() / 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50651e3",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1f4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze class imbalance\n",
    "print('Class distribution:')\n",
    "print(fraud_df['class'].value_counts())\n",
    "\n",
    "# Prepare features: drop identifiers and non-numeric columns\n",
    "drop_cols = ['class', 'ip_address', 'signup_time', 'purchase_time', 'user_id', 'device_id', 'ip_int']\n",
    "X = fraud_df.drop(drop_cols, axis=1)\n",
    "y = fraud_df['class']\n",
    "\n",
    "# One-hot encode categorical features\n",
    "categorical = ['source', 'browser', 'sex', 'country']\n",
    "X = pd.get_dummies(X, columns=categorical, drop_first=True)\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Apply SMOTE to training data only\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "print('After SMOTE:', np.bincount(y_train_res))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
